# -*- coding: utf-8 -*-
"""LAB1andLAB2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17JpNyWETawzcWtWLzPNkA7npu6_Ae0GK

# LAB-01
Create and implement a basic neuron model within a computational framework. integrate essential elements like input nodes, weights, bias and activation fumction (sigmoid, ReLU, hyperbolic tangent). to construct a comprehensive representation of neuron. evaluate and observe how each activation function influence the network behaviour and effectiveness in handling different types and data
"""

import math
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
  y=1.0/(1+math.exp(-x))
  return y

def relu(x):
  return max(0,x)

def tanh(x):
  return np.tanh(x)

def linear(x):
  return x

def unit(x):
  return 1 if x>0 else 0

def sign(x):
  return 1 if x>0 else (-1 if x<0 else 0)

def activate(inp, weight):
  h=0
  for x,w in zip(inp, weight):
    h+=x*w
  return [sigmoid(h), relu(h), tanh(h), linear(h), unit(h), sign(h)]

if __name__=="__main__":
  inp=[.5, .3, .2]
  weight=[.4, .7, .2]
  output=activate(inp, weight)
  print(f"Sigmoid: {output[0]}")
  print(f"ReLU: {output[1]}")
  print(f"tanh: {output[2]}")
  print(f"linear: {output[3]}")
  print(f"unit: {output[4]}")
  print(f"sign: {output[5]}")

x_values = np.linspace(-10, 10, 400)
def plot_activation(activation, name):
    y_values = [activation(x) for x in x_values]
    plt.figure(figsize=(8, 5))
    plt.plot(x_values, y_values, label=name,color='lightcoral')
    plt.title(f'{name} Activation Function')
    plt.xlabel('Input')
    plt.ylabel('Output')
    plt.grid()
    plt.legend()
    plt.show()
    print()

plot_activation(sigmoid, 'Sigmoid')
plot_activation(relu, 'ReLU')
plot_activation(tanh, 'Tanh')
plot_activation(linear, 'Linear')
plot_activation(unit, 'Unit Step')
plot_activation(sign, 'Sign')

"""# **LAB-02
Develop and Implement a program to execute the perceptron learning algorithm customised to train a single layer perceptron for binary classification task. Create a robust algorithm that refines the model weight iteratively, resulting in a profecient single layer perceptron capable of effectively handing binary classification task/ challenges.**
"""

import pandas as pd
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def predict(X, weights):
    z = np.dot(X, weights)
    return sigmoid(z)

def binary_output(y_prob):
    return [1 if y >= 0.5 else 0 for y in y_prob]

def train_perceptron(X, y, learning_rate=0.01):
    X = np.insert(X, 0, 1, axis=1)
    weights = np.zeros(X.shape[1])
    flag = False
    while not flag:
        flag = True
        for i in range(len(X)):
            z = np.dot(X[i], weights)
            y_pred = sigmoid(z)
            error = y[i] - y_pred
            if (y[i] == 1 and y_pred < 0.5) or (y[i] == 0 and y_pred >= 0.5):
                flag = False
                weights += learning_rate * error * X[i]
    return weights

def evaluate(X, y, weights):
    X = np.insert(X, 0, 1, axis=1)
    y_prob = predict(X, weights)
    y_pred = binary_output(y_prob)
    accuracy = np.mean(y_pred == y)
    return accuracy

def classify_new_input(new_input, weights):
    new_input = np.array(new_input)
    new_input_with_bias = np.insert(new_input, 0, 1)
    y_prob = sigmoid(np.dot(new_input_with_bias, weights))
    return 1 if y_prob >= 0.5 else 0

data = pd.read_csv("data.csv")

y = data.iloc[:, 0].values
X = data.iloc[:, 1:].values

learning_rate = 0.5
weights = train_perceptron(X, y, learning_rate)

accuracy = evaluate(X, y, weights)

bias = weights[0]
feature_weights = weights[1:]

print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Bias (x0): {bias}")
print(f"Weights: {feature_weights}")

new_input = [3.4, 5.7]
predicted_class = classify_new_input(new_input, weights)

print(f"Predicted class for the new input {new_input}: {predicted_class}")

